{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "第8章Attention.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNvR1UjXp3bR5twplI5CaPG"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "SRgpeJBUSWln"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoderの改良①"
      ],
      "metadata": {
        "id": "EYsVCQ0vTBrT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "85RJumGqR2r3"
      },
      "outputs": [],
      "source": [
        "class WeightSum:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.cache = None\n",
        "    \n",
        "    def forward(self, hs, a):\n",
        "        N, T, H = hs.shape\n",
        "        \n",
        "        ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
        "        t = hs * ar\n",
        "        c = np.sum(t, axis=1)\n",
        "\n",
        "        self.cache = (hs, ar)\n",
        "        return c\n",
        "    \n",
        "    def backward(self, dc):\n",
        "        hs, ar = self.cache\n",
        "        N, T, H = self.cache\n",
        "\n",
        "        dt = dc.reshape(N, 1, H).repeat(T, axis=1) # sumの逆伝播\n",
        "        dar = dt * hs\n",
        "        dhs = dt * ar\n",
        "        da = np.sum(dar, axis=2) # repeatの逆伝播\n",
        "\n",
        "        return dhs, da"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoderの改良②"
      ],
      "metadata": {
        "id": "XDvycSi2S-LQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x - x.max(axis=1, keepdims=True)\n",
        "        x = np.exp(x)\n",
        "        x /= x.sum(axis=1, keepdims=True)\n",
        "    elif x.ndim == 1:\n",
        "        x = x - np.max(x)\n",
        "        x = np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "4oXsdNn8TKCk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Softmax:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.out = softmax(x)\n",
        "        return self.out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = self.out * dout\n",
        "        sumdx = np.sum(dx, axis=1, keepdims=True)\n",
        "        dx -= self.out * sumdx\n",
        "        return dx"
      ],
      "metadata": {
        "id": "K2KiKbsCTnn4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionWeight:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.softmax = Softmax()\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, hs, h):\n",
        "        N, T, H = hs.shape\n",
        "\n",
        "        hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
        "        t = hs * hr\n",
        "        s = np.sum(t, axis=2)\n",
        "        a = self.softmax.forward(s)\n",
        "        \n",
        "        self.cache = (hs, hr)\n",
        "        return a\n",
        "    \n",
        "    def backward(self, da):\n",
        "        hs, hr = self.cache\n",
        "        N, T, H = hs.shape\n",
        "\n",
        "        ds = self.softmax.backward(da)\n",
        "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
        "        dhs = dt * hr\n",
        "        dhr = dt * hs\n",
        "        dh = np.sum(dhr, axis=1)\n",
        "\n",
        "        return dhs, dh"
      ],
      "metadata": {
        "id": "JlVcYtcyTSVH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoderの改良③"
      ],
      "metadata": {
        "id": "U2Uw9_jFUdRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.attention_weight_layer = AttentionWeight()\n",
        "        self.weight_sum_layer = WeightSum()\n",
        "        self.attention_weight = None\n",
        "    \n",
        "    def forward(self, hs, h):\n",
        "        a = self.attention_weight_layer.forward(hs, h)\n",
        "        out = self.weight_sum_layer.forward(hs, a)\n",
        "        self.attention_weight = a\n",
        "        return out\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
        "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
        "        dhs = dhs0 + dhs1\n",
        "        return dhs, dh"
      ],
      "metadata": {
        "id": "w-_0eCfEUvEv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeAttention:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.layers = None\n",
        "        self.attention_weight = None\n",
        "    \n",
        "    def forward(self, hs_enc, hs_dec):\n",
        "        N, T, H = hs_dec.shape\n",
        "        out = np.empty_like(hs_dec)\n",
        "        self.layers = []\n",
        "        self.attention_weight = []\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = Attention()\n",
        "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:, t, :])\n",
        "            self.layers.append(layer)\n",
        "            self.attention_weight.append(layer.attention_weight)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        N, T, H = dout.shape\n",
        "        dhs_enc = 0\n",
        "        dhs_dec = np.empty_like(dout)\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = self.layers[t]\n",
        "            dhs, dh = layer.backward(dout[:, t, :])\n",
        "            dhs_enc += dhs\n",
        "            dhs_dec[:, t, :] = dh\n",
        "        \n",
        "        return dhs_enc, dhs_dec"
      ],
      "metadata": {
        "id": "QNjMZf2rVnlt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9YNQKf1iW8SS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}